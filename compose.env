# env file example for plugin execution of jobworkerp-rs (libembedding_runner.so, libllm_runner.so, libwhisper_runner.so)
#
# Notice: ALL TIME UNIT WITHOUT EXPLICITLY SPECIFIED IS MILLISECONDS

############################
# jobworkerp storage
############################

# storage options (hybrid recommended, not recommended redis, rdb now)
STORAGE_TYPE = hybrid

# db setting (USING STORAGE_TYPE={hybrid,rdb})
# (use sqlite3 for test in single instance. no settings)

## mysql seting example (for using in multiple instances)
#MYSQL_HOST = "localhost"
#MYSQL_PORT = "3306"
#MYSQL_USER = "mysql"
#MYSQL_PASSWORD = "mysql"
#MYSQL_DBNAME = "test"
#MYSQL_MAX_CONNECTIONS = 20


# redis setting (USING STORAGE_TYPE={hybrid,redis})
# (not available for redis cluster (clustered pubsub not supported by redis-rs))
REDIS_USERNAME  = ""
REDIS_PASSWORD  = ""
REDIS_URL = "redis://redis:6379"
REDIS_POOL_CREATE_TIMEOUT_MSEC = 5000
REDIS_POOL_WAIT_TIMEOUT_MSEC = 60000
REDIS_POOL_RECYCLE_TIMEOUT_MSEC = 5000
REDIS_POOL_SIZE = 20

# memory cache setting (stretto: consider cost of a record as 1)
MEMORY_CACHE_NUM_COUNTERS = 12960
MEMORY_CACHE_MAX_COST = 1296
MEMORY_CACHE_USE_METRICS = true

########################################################
# job queueing, recovery from rdb (hybrid only)
########################################################

# seconds for expiring DIRECT or LISTEN_AFTER job result (storage_type=hybrid (or redis))
JOB_QUEUE_EXPIRE_JOB_RESULT_SECONDS = 3600
# fetch interval msecs for periodic or run_after job
JOB_QUEUE_FETCH_INTERVAL = 1000

# restore jobs from rdb at startup
# (ex. use to restore jobs remaining when the worker process panics and crashes)
# (same function as JobRestoreService/Restore)
# only enable when STORAGE_TYPE = hybrid
STORAGE_RESTORE_AT_STARTUP = false

# concurrency for default channel (worker.channel=None)
WORKER_DEFAULT_CONCURRENCY = 1

# additional queue channel
# (separate by comma)
# worker process fetch jobs only from specified channels.
WORKER_CHANNELS=quad,single
# worker channels concurrency (same sequence as WORKER_CHANNELS)
WORKER_CHANNEL_CONCURRENCIES=4,1

############################
# logging, metrics
############################
# trace, debug, info, warn, error
LOG_LEVEL=info
# log output file path
LOG_FILE_DIR = /home/jobworkerp/log/
# log file format to json
LOG_USE_JSON = true
# output stdout or not
LOG_USE_STDOUT = true

# can specify each one: JAEGER_ADDR or ZIPKIN_ADDR (OTLP_ADDR is under testing...)
#JAEGER_ADDR="jaeger.istio-system.svc.cluster.local:6831"
#ZIPKIN_ADDR="http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans"
#OTLP_ADDR="http://otel-collector.default.svc.cluster.local:4317" # UNDER TESTING

############################
# front server
############################

# grpc listen addr
GRPC_ADDR=0.0.0.0:9000

# use grpc web (for connect from js etc)
USE_GRPC_WEB=false


############################
# worker(runner) specific settings
############################

# for docker runner
# docker GID. use value: $(stat -c '%g' /var/run/docker.sock)
DOCKER_GID=963

# plugin directory (find *.so in this dir)
PLUGINS_RUNNER_DIR=/home/jobworkerp/plugins

## for slack worker (notification in result specify as next_workers="-1" in worker definition)
#SLACK_TITLE="Result Notification"
#SLACK_BOT_TOKEN="FILLIN_YOUR_TOKEN"
#SLACK_CHANNEL=#random
#SLACK_NOTIFY_SUCCESS=true
#SLACK_NOTIFY_FAILURE=true


# hugging face local cache dir
HF_HOME=/home/jobworkerp/cache

#################################
# LLM plugin (libllm_runner.so)
#################################

# pre-setting profile: llama-7b-v2, elyza-7b-fast, stable-lm-ja-vocab-beta-7b, stable-lm-ja-gamma-7b, stable-lm-ja-3b-4e1t
#HF_MODEL_LOADER_PROFILE=elyza-7b-fast
#HF_MODEL_LOADER_PROFILE=stable-lm-ja-gamma-7b
HF_MODEL_LOADER_PROFILE=stable-lm-ja-3b-4e1t

# manual setting examples (huggingface): enable if HF_MODEL_LOADER_PROFILE is not defined
# (llama, falcon, stable-lm, mistral)
# (mistral model may not work properly(only work with predefined configuration in candle))
# model name (huggingface model hub)
# if not defined, load local files
#LLM_MODEL_ID="stabilityai/japanese-stablelm-3b-4e1t-base"
# model revision (huggingface model hub) (option: default=main)
#LLM_REVISION=
## tokenizer filename (local file path or hugging face filename. default: tokenizer.json)
##LLM_TOKENIZER_FILE=/home/jobworkerp/.cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/tokenizer.json
## safetensors model filename (local file path or hugging face filename. default: model.safetensors)
##LLM_WEIGHT_FILES=/home/jobworkerp/.cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/model.safetensors
# model config filename (local file path or hugging face filename. default: config.json)
##LLM_MODEL_CONFIG_FILE=/home/jobworkerp/.cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/config.json

# available for llama and mistral, stable_lm
#LLM_USE_FLASH_ATTN=true
# eos token (depends on the tokenizer and the model)
#LLM_EOS_TOKEN=<|endoftext|>
## Run on CPU rather than on GPU.
#LLM_USE_CPU=false

# quantized model is now not supported
#LLM_QUANTIZED=false

#################################
# sentence embedding plugin (libembedding_runner.so) (Bert)
#################################

# use cpu
BERT_USE_CPU=false
# The number of times to run the prompt.
BERT_N=1
# L2 normalization for embeddings.
BERT_NORMALIZE_EMBEDDINGS=false
# Use tanh based approximation for Gelu instead of erf implementation.
BERT_APPROXIMATE_GELU=true
# huggingface model id
BERT_MODEL_ID="intfloat/multilingual-e5-small"
# sentence prefix for embedding (depends on model)
BERT_PREFIX="query: "

# delimiter and parenthes for sentence splitting (default for japanese sentences)
SENTENCE_SPLITTER_DELIMITER_CHARS="。．！？!?\n"
SENTENCE_SPLITTER_PARENTHESE_PAIRS=「」,『』,【】


#################################
# Whisper (libwhisper_runner.so)
#################################

WHISPER_USE_CPU=false
# not tested...
WHISPER_QUANTIZED=false

#  one of `Tiny`, `TinyEn`, `Base`, `BaseEn`, `Small`, `SmallEn`, `Medium`, `MediumEn`, `Large`, `LargeV2`, `LargeV3`, `DistilMediumEn`, `DistilLargeV2`
WHISPER_MODEL="LargeV3"
