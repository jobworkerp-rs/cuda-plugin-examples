# env file example for command line execution (embedding-cli, llm-cli, whisper-cli)
#
#################################
# LLM plugin
#################################

# pre-setting profile: llama-7b-v2, elyza-7b-fast, stable-lm-ja-vocab-beta-7b, stable-lm-ja-gamma-7b, stable-lm-ja-3b-4e1t
#HF_MODEL_LOADER_PROFILE=elyza-7b-fast
#HF_MODEL_LOADER_PROFILE=stable-lm-ja-gamma-7b
HF_MODEL_LOADER_PROFILE=stable-lm-ja-3b-4e1t

# manual setting (huggingface): llama, falcon, stable-lm, mistral
# (mistral model may not work properly(only work with predefined configuration in candle))
#
# hugging face local cache dir
#HF_HOME=/mnt/cache/huggingface/

# model name (huggingface model hub)
# if not defined, load local files
#LLM_MODEL_ID="stabilityai/japanese-stablelm-3b-4e1t-base"
# model revision (huggingface model hub) (option: default=main)
#LLM_REVISION=

# tokenizer filename (local file path or hugging face filename. default: tokenizer.json)
#LLM_TOKENIZER_FILE=/path/to/cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/tokenizer.json
# weight filename (local file path or hugging face filename. default: model.safetensors)
#LLM_WEIGHT_FILES=/path/to/mnt/cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/model.safetensors
# model config filename (local file path or hugging face filename. default: config.json)
#LLM_MODEL_CONFIG_FILE=/path/to/mnt/cache/huggingface/hub/models--stabilityai--japanese-stablelm-3b-4e1t-base/snapshots/e98f084363b77b2e628e3cbcc91a9fd8c7f55b3f/config.json

# available for llama and mistral, stable_lm
#LLM_USE_FLASH_ATTN=true
# eos token (depends on the tokenizer and the model)
#LLM_EOS_TOKEN=<|endoftext|>
## Run on CPU rather than on GPU.
#LLM_USE_CPU=false

# quantized model is now not supported
#LLM_QUANTIZED=false

#################################
# sentence embedding plugin (Bert)
#################################

# use cpu
BERT_USE_CPU=false
# The number of times to run the prompt.
BERT_N=1
# L2 normalization for embeddings.
BERT_NORMALIZE_EMBEDDINGS=false
# Use tanh based approximation for Gelu instead of erf implementation.
BERT_APPROXIMATE_GELU=true
# huggingface model id
BERT_MODEL_ID="intfloat/multilingual-e5-small"
# sentence prefix for embedding (depends on model)
BERT_PREFIX="query: "

# for spliting into sentences (for japanese sentence)
SENTENCE_SPLITTER_DELIMITER_CHARS="。．！？!?\n"
SENTENCE_SPLITTER_PARENTHESE_PAIRS=「」,『』,【】


#################################
# Whisper
#################################

WHISPER_USE_CPU=false
WHISPER_QUANTIZED=false
#  one of `Tiny`, `TinyEn`, `Base`, `BaseEn`, `Small`, `SmallEn`, `Medium`, `MediumEn`, `Large`, `LargeV2`, `LargeV3`, `DistilMediumEn`, `DistilLargeV2`
WHISPER_MODEL="LargeV3"

#################################
# jobworkerp-rs setting (minimum setting: sqlite, no redis. not recommended)
# (recommend to use docker compose with hybrid storage setting (redis and rdb))
#################################
STORAGE_TYPE=rdb
WORKER_CONCURRENCY=1

LOG_LEVEL=INFO
LOG_FILE_DIR=/home/jobworkerp/log/
LOG_USE_JSON=true
LOG_USE_STDOUT=true

