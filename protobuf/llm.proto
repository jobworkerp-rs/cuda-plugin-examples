syntax = "proto3";

package llm;

message InferenceRequest {
    // The initial prompt.
    string prompt = 1;

    // The length of the sample to generate (in tokens).
    uint32 sample_len = 2;

    // The temperature used to generate samples.
    optional double temperature= 3;

    // Nucleus sampling probability cutoff.
    optional double top_p = 4;

    // Penalty to be applied for repeating tokens, 1. means no penalty.
    optional float repeat_penalty= 5;

    // The context size to consider for the repeat penalty.
    optional uint32 repeat_last_n = 6;

    // The seed to use when generating random samples.
    optional uint64 seed = 7;

    // print iteratively (false for plugin runner, true for cli)
    bool need_print= 8;
}

message InferenceResponse {
    // The generated text.
    string text = 1;
}
